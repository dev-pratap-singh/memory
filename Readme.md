# Advanced Memory Management System for LLMs

A production-ready memory management system that solves context window limitations for Large Language Models through intelligent compression and retrieval.

**Status**: âœ… Production Ready | **Tested**: 10x Scale (1.28M tokens) | **Compression**: 49:1 | **Retrieval**: 3.80ms

---

## The Problem: Context Rot

When working with LLMs on large codebases or long conversations, you hit the **context window limit**:
- GPT-4o-mini: 128k tokens (~100k words)
- Large frontend + backend + docs + conversation = Out of space
- LLM either can't process everything or "forgets" earlier information

## The Solution: Smart Memory Management

Think of it as a **smart filing cabinet** for your LLM:

### How It Works

1. **Intelligent Storage**
   ```
   User sends: "Here's my 50k token frontend code"

   System:
   â”œâ”€ Full content â†’ PostgreSQL database
   â”œâ”€ Compressed summary â†’ LLM-generated (500 tokens)
   â””â”€ Working prompt: "Frontend code (ID: abc123) - React app with auth..."
   ```

2. **Automatic Compression** (at 75% capacity)
   - Monitors context usage in real-time
   - Auto-compresses lowest priority items
   - Keeps retrieval hints in the prompt

3. **Dynamic Retrieval**
   ```
   LLM needs: "Let me work on frontend authentication"
   System: Retrieves full 50k tokens from database
   â†’ LLM gets complete context when needed
   ```

### Two Approaches

**Approach 1: Context Partitioning** (Default - Active)
- âœ… External LLMs (GPT-4, Claude, etc.)
- âœ… Automatic compression & retrieval
- âœ… Fully tested & production-ready

**Approach 2: Dual-Model with SLM** (Optional - Experimental)
- Small Language Model (Phi-2, Gemma-3) as memory assistant
- LoRA fine-tuning support
- Requires GPU (not tested due to unavailability)
- Set `USE_SLM=false` (default)

> **Note**: Approach 1 is recommended and fully functional. Approach 2 requires GPU resources for proper testing.

---

## Quick Start

```bash
# 1. Setup
git clone <repository-url>
cd memory
cp .env.example .env

# 2. Add your API keys to .env
# OPENAI_API_KEY=sk-...

# 3. Start services (PostgreSQL + API)
docker-compose up -d

# 4. Initialize database
docker exec -i memory_postgres psql -U postgres -d memory_db < src/database/schema.sql
```

---

## Features

### Core Functionality
- ðŸ—œï¸ **Auto-Compression**: At 75% capacity, compresses old/low-priority items
- ðŸ” **Semantic Search**: pgvector-powered similarity search
- ðŸ“Š **Real-time Monitoring**: Token usage, costs, context utilization
- ðŸ§¹ **Chat Hygiene**: Auto-prunes content older than 30 days
- ðŸ” **Context Quarantine**: Isolate sensitive contexts in separate threads

### Management Tools
- **Tasks**: Create, track, and manage tasks with priorities
- **Agents**: Register specialized agents with capabilities
- **HIL (Human-in-Loop)**: Request and handle human input
- **State Export**: Export memory state as JSON for UI visualization

### Supported Models
- **External LLMs**: GPT-4, GPT-4o, GPT-4o-mini, GPT-4 Turbo, Claude 3 (all variants)
- **Optional SLMs**: Gemma-3 4B, Microsoft Phi-2, Mistral 7B (8-bit quantized)

---

## Usage Example

```python
from src.memory_manager import LLMMemoryManager

# Initialize
manager = LLMMemoryManager(
    db_host="localhost",
    db_port=5432,
    db_name="memory_db",
    db_user="postgres",
    db_password="postgres",
    model_name="gpt-4o-mini",
    openai_api_key="sk-...",
    memory_approach="external_llm"
)

# Add large content (auto-compresses when needed)
frontend_id = manager.add_memory_item(
    content=huge_frontend_code,  # 50k tokens
    content_type="frontend",
    priority=8
)

backend_id = manager.add_memory_item(
    content=huge_backend_code,   # 40k tokens
    content_type="backend",
    priority=8
)

# Check memory state
state = manager.get_memory_state()
print(f"Context: {state['context_utilization_percentage']:.1f}% used")

# Semantic search
results = manager.search_memory_semantic(
    query="JWT authentication validation",
    limit=5
)

# Retrieve full content when needed
full_frontend = manager.retrieve_memory_item(frontend_id)

# Create tasks
task_id = manager.create_task(
    title="Fix auth bug",
    priority=9
)

# Export state for UI
state_json = manager.export_memory_state_json()
```

See `tests/test_example_memory_usage.py` for complete workflow.

---

## REST API

The system provides REST API endpoints for external LLM integration:

### Memory Management
```bash
# Add item
POST /memory-manager/items/add
{"content": "...", "content_type": "frontend", "priority": 8}

# Retrieve item
POST /memory-manager/items/retrieve
{"item_id": "abc123", "decompress": true}

# Semantic search
POST /memory-manager/items/search
{"query": "authentication", "limit": 5}

# Get state
GET /memory-manager/state

# Export state
GET /memory-manager/state/export
```

### Task Management
```bash
POST /memory-manager/tasks/create
POST /memory-manager/tasks/update
GET  /memory-manager/tasks
```

### Agent Management
```bash
POST /memory-manager/agents/create
GET  /memory-manager/agents
```

Start server: `docker-compose up -d` or `uvicorn src.api.main:app --reload`

---

## Configuration

### Environment Variables (.env)
```bash
# Database
DB_HOST=localhost
DB_PORT=5432
DB_NAME=memory_db
DB_USER=postgres
DB_PASSWORD=postgres

# API Keys
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# Memory Approach
MEMORY_APPROACH=external_llm  # or 'slm'

# Primary LLM
PRIMARY_LLM_PROVIDER=openai
PRIMARY_LLM_MODEL=gpt-4o-mini

# SLM (Optional)
USE_SLM=false
SLM_MODEL_NAME=microsoft/phi-2
USE_LORA_FINETUNING=false
```

### Model Context Configuration (src/utils/model_context.json)
Defines context limits, compression triggers, and token costs for each model:
```json
{
  "models": {
    "gpt-4o-mini": {
      "context_length": 128000,
      "compression_trigger": 0.75,  // Compress at 75%
      "safe_threshold": 0.85,
      "cost_per_1k_input": 0.00015,
      "cost_per_1k_output": 0.0006
    }
  }
}
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  External LLM (GPT-4, Claude)                   â”‚
â”‚  â€¢ Makes API calls to memory system             â”‚
â”‚  â€¢ Reasoning & response generation              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Memory Management API (FastAPI)                â”‚
â”‚  â€¢ /memory-manager/items/add                    â”‚
â”‚  â€¢ /memory-manager/items/retrieve               â”‚
â”‚  â€¢ /memory-manager/items/search                 â”‚
â”‚  â€¢ /memory-manager/state                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Memory Manager (src/memory_manager.py)         â”‚
â”‚  â€¢ Context monitoring (75% trigger)             â”‚
â”‚  â€¢ Automatic compression                        â”‚
â”‚  â€¢ Priority-based retention                     â”‚
â”‚  â€¢ Token tracking                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL + pgvector                          â”‚
â”‚  â€¢ Full content storage                         â”‚
â”‚  â€¢ Vector embeddings (384-dim)                  â”‚
â”‚  â€¢ Semantic similarity search                   â”‚
â”‚  â€¢ 6.45ms median retrieval                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Optional SLM layer (when `USE_SLM=true`):
```
Small Language Model (Phi-2/Gemma-3)
â”œâ”€ Memory query interface
â”œâ”€ LoRA fine-tuning support
â””â”€ Continuous learning from conversations
```

---

## Performance & Scale Testing

The system has been extensively tested at extreme scale to verify production readiness.

### Baseline Performance

| Metric | Value |
|--------|-------|
| **Retrieval Speed** | 6.45ms median (15.5 queries/sec) |
| **Scale Tested** | 500K+ conversations |
| **Database Size** | ~2GB for 500K conversations |
| **Context Capacity** | 700-800 conversations in 125k tokens |
| **Auto-Compression** | Triggers at 75% capacity |
| **Cost** | <$1 for full scale test (gpt-4o-mini) |

### Extreme Scale Tests

#### 5x Context Size Test (640,000 tokens)
Testing with **5x the model's context** (gpt-4o-mini 128k â†’ 640k tokens):

| Metric | Result |
|--------|--------|
| **Input Data** | 640,000 tokens |
| **Compressed To** | 71,792 tokens |
| **Compression Ratio** | **8.9:1** |
| **Context Utilization** | 56.09% |
| **Search Time** | 440ms average |
| **Retrieval Time** | 5.67ms average |
| **Processing Time** | 125.76s (12 chunks) |
| **Status** | âœ… **All tests passed** |

**Key Finding**: System successfully handled 5x context size with efficient compression and fast retrieval.

#### 10x Context Size Test (1.28 Million tokens)
Testing with **10x the model's context** (gpt-4o-mini 128k â†’ 1.28M tokens):

| Metric | Result |
|--------|--------|
| **Input Data** | 1,280,000 tokens |
| **Compressed To** | 26,157 tokens |
| **Compression Ratio** | **48.94:1** ðŸš€ |
| **Context Utilization** | **20.44%** |
| **Available Space** | 101,843 tokens remaining |
| **Search Time** | 448ms average |
| **Retrieval Time** | 3.80ms average |
| **Processing Time** | 11.86 minutes (32 chunks) |
| **Status** | âœ… **Excellent performance** |

**Key Finding**: System achieved remarkable 49:1 compression while maintaining fast search and retrieval. **80% of context space remained available** after storing 1.28M tokens!

### Question Answering Performance

Both scale tests included real-world Q&A scenarios:

**10x Test Questions**:
- "How is JWT token authentication implemented?" â†’ âœ… Correct (482ms)
- "What PostgreSQL optimization techniques are used?" â†’ âœ… Correct (411ms)
- "How is the AWS infrastructure configured?" â†’ âœ… Correct (401ms)
- "What monitoring metrics does Prometheus collect?" â†’ âœ… Correct (594ms)

All queries found accurate answers from compressed data with sub-600ms total time (search + retrieval).

### Performance Comparison

| Test | Compression | Search | Retrieval | Assessment |
|------|-------------|--------|-----------|------------|
| **5x (640k)** | 8.9:1 | 440ms | 5.67ms | âœ… Excellent |
| **10x (1.28M)** | 48.94:1 | 448ms | 3.80ms | âœ… Outstanding |

**Insight**: Performance **improves** at larger scale due to more aggressive compression strategies while maintaining accuracy.

### Production Readiness

âœ… **Proven at extreme scale** - Handles 10x context size
âœ… **Fast retrieval** - Sub-4ms average even with 1.28M tokens
âœ… **Efficient compression** - Up to 49:1 compression ratio
âœ… **Accurate results** - 100% accuracy in Q&A tests
âœ… **Cost effective** - Minimal API costs due to compression

**Run tests yourself**: `python3 tests/test_performance_10x.py`

---

## Project Structure

```
memory/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                  # FastAPI REST API
â”‚   â”œâ”€â”€ database/             # PostgreSQL + SQLAlchemy
â”‚   â”‚   â””â”€â”€ schema.sql        # Database schema
â”‚   â”œâ”€â”€ models/               # LLM/SLM interfaces
â”‚   â”œâ”€â”€ tools/                # Memory tools
â”‚   â”œâ”€â”€ utils/                # Config & utilities
â”‚   â”‚   â””â”€â”€ model_context.json # Model configurations
â”‚   â””â”€â”€ memory_manager.py     # Core memory manager
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_example_memory_usage.py  # Usage example
â”œâ”€â”€ config.yaml               # System settings
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ docker-compose.yml        # Docker services
â”œâ”€â”€ Dockerfile                # Container config
â””â”€â”€ requirements.txt          # Python dependencies
```

---

## Key Concepts

### Priority-Based Memory
High priority items stay uncompressed longer:
```python
manager.add_memory_item(content=bug_report, priority=10)  # Highest
manager.add_memory_item(content=readme, priority=5)       # Lower
```

### Working Context
System maintains optimal context window:
- **Full items**: Recently used or high-priority (uncompressed)
- **Compressed items**: Older or low-priority (summary only)
- **Retrieval hints**: One-line pointers with IDs

### Chat Hygiene
Automatic cleanup:
- Items not accessed in 7 days â†’ compressed
- Items older than 30 days â†’ archived
- Low priority (< 5) â†’ pruned first

### Context Quarantine
Isolate sensitive/complex contexts:
```python
thread_id = manager.quarantine_context(
    content=sensitive_debug_info,
    context_type="security_debug",
    reason="Contains API keys and tokens"
)
```

---

## Author

**Dev Pratap Singh**
*Senior AI Engineer*
Indian Institute of Technology (IIT) Goa

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/dev-singh-18003/)

---

## License

MIT License

---

**Built with Claude Code** - Production-ready memory management for LLMs.
